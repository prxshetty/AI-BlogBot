{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac13e8a-a25e-4be3-8aec-8b569a31f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from urllib.parse import urljoin\n",
    "from requests_html import HTMLSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a57428-b233-4dba-b75e-6ff25cf84a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#pegasus load\n",
    "model_name='google/pegasus-xsum'\n",
    "tokenizer= PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34380fdd-243f-4695-a307-7602f738bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url='https://techcrunch.com'\n",
    "relative_url='/category/artificial-intelligence'\n",
    "url=urljoin(base_url, relative_url)\n",
    "page= requests.get(url)\n",
    "soup=BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c5ed4b5-7241-4644-aec6-e3b23dced49f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_headlines(base_url, relative_url):\n",
    "    try:\n",
    "        absolute_url = urljoin(base_url, relative_url)\n",
    "        session = HTMLSession()\n",
    "        response = session.get(absolute_url)\n",
    "        response.raise_for_status() \n",
    "        # Find all elements containing both headline text and article URLs\n",
    "        headlines_elements = response.html.find('h2.post-block__title a')\n",
    "        # Extract headline text and article URLs as tuples\n",
    "        headlines = [(headline.text, headline.absolute_links.pop()) for headline in headlines_elements]\n",
    "        return headlines\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching headlines from URL '{absolute_url}': {e}\")\n",
    "        return None\n",
    "\n",
    "def get_article_content(article_url):\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(article_url)\n",
    "        response.raise_for_status()\n",
    "        content_element = response.html.find('div.article-content', first=True)\n",
    "        if content_element:\n",
    "            content = content_element.text\n",
    "            return content\n",
    "        else:\n",
    "            print(f\"Content element not found in URL '{article_url}'\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching content from URL '{article_url}': {e}\")\n",
    "        return None\n",
    "\n",
    "def get_paraphrased_content(original_content):\n",
    "    try:\n",
    "        print(\"Original Content Length:\", len(original_content))\n",
    "        # Split the original content into paragraphs\n",
    "        paragraphs = original_content.split('\\n\\n')  # Assuming paragraphs are separated by double line breaks\n",
    "        paraphrased_paragraphs = []\n",
    "        \n",
    "        # Paraphrase each paragraph\n",
    "        for paragraph in paragraphs:\n",
    "            # Split each paragraph into chunks of 512 tokens\n",
    "            chunks = [paragraph[i:i+512] for i in range(0, len(paragraph), 512)]\n",
    "            paraphrased_chunks = []\n",
    "            \n",
    "            # Paraphrase each chunk\n",
    "            for chunk in chunks:\n",
    "                inputs = tokenizer([chunk], return_tensors='pt', max_length=512, truncation=True)\n",
    "                summary_ids = model.generate(inputs['input_ids'], num_beams=4, min_length=30, max_length=512, temperature=0.7, do_sample=True, early_stopping=True)\n",
    "                paraphrased_chunk = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "                paraphrased_chunks.append(paraphrased_chunk)\n",
    "                \n",
    "            # Combine the paraphrased chunks into a single string representing the paraphrased paragraph\n",
    "            paraphrased_paragraph = ' '.join(paraphrased_chunks)\n",
    "            paraphrased_paragraphs.append(paraphrased_paragraph)\n",
    "        \n",
    "        # Combine the paraphrased paragraphs into a single string representing the paraphrased content\n",
    "        paraphrased_content = '\\n\\n'.join(paraphrased_paragraphs)\n",
    "        \n",
    "        # Check if the paraphrased content is not empty\n",
    "        if paraphrased_content.strip():\n",
    "            return paraphrased_content\n",
    "        else:\n",
    "            print(\"Paraphrased content is empty\")\n",
    "            return original_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching or paraphrasing content: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3bd568-01aa-4254-9953-634f7b93f223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Microsoft’s Windows 11 Copilot gets smarter with new plugins and skills\n",
      "1. With Brain.ai, generative AI is the OS\n",
      "2. Former Twitter engineers are building Particle, an AI-powered news reader, backed by $4.4M\n",
      "3. Brave’s Leo AI assistant is now available to Android users\n",
      "4. Google brings Stack Overflow’s knowledge base to Gemini for Google Cloud\n",
      "5. Venus Williams brings her interior design skills to Palazzo, a new generative AI-powered platform\n",
      "6. Mark Zuckerberg woos Big Tech in Asia to double down on AI chips\n",
      "7. Gemini on Android can’t ID songs, and it’s frustrating\n",
      "8. Tim Cook says Apple will ‘break new ground’ in GenAI this year\n",
      "9. Morph Studio lets you make films using Stability AI–generated clips\n",
      "10. Anamorph’s generative technology reorders scenes to create unlimited versions of one film\n",
      "11. Adobe reveals a GenAI tool for music\n",
      "12. Microsoft invests in yet another AI company\n",
      "13. StarCoder 2 is a code-generating AI that runs on most GPUs\n",
      "14. SambaNova now offers a bundle of generative AI models\n",
      "15. Yolk is a social app where users swap custom live stickers — no text allowed\n",
      "16. Diffusion transformers are the key behind OpenAI’s Sora — and they’re set to upend GenAI\n",
      "17. Lightricks announces AI-powered filmmaking studio to help creators visualize stories\n",
      "18. AIs serve up ‘garbage’ to questions about voting and elections\n",
      "19. GitHub’s Copilot Enterprise is now generally available at $39 a month\n"
     ]
    }
   ],
   "source": [
    "#Titles with their indexes\n",
    "headlines=get_headlines(base_url, relative_url)\n",
    "if headlines:\n",
    "    for i, (headline,_) in enumerate(headlines, 0):\n",
    "        print(f\"{i}. {headline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cff7144e-4ab2-4122-88fc-dc1a789a5241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Title: With Brain.ai, generative AI is the OS\n",
      "\n",
      " Content: The Humane Ai Pin and Rabbit handheld have captured a good bit of press interest for their individual approaches to integrating generative AI with hardware. Humane, in particular, is presenting its wearable as a look at life beyond the smartphone. That naturally prompts the question: What, precisely, is wrong with the smartphone? While it’s true that the form factor has plateaued, these devices are still out in the world, in billions of hands.\n",
      "Earlier this week, I met with Jerry Yue amid the cacophonous din of Deutsch Telekom’s Mobile World Congress booth. After a product demo and a sit-down conversation, I admit that I’m impressed with the Brain.ai (alternately known as Brain Technologies) founder and CEO’s vision for the future of smartphones. I won’t go so far as saying I’m fully convinced until I’ve had an opportunity to spend more time with the product, but it absolutely paints a compelling picture of how generative AI might be foundational to the next generation of devices.\n",
      "The whole “future of smartphones” bit may be hyperbolic, but at the very least, I suspect some of the biggest names in the biz are currently studying the way first-party generative AI effectively forms the backbone of the product’s operating system. But while phone companies may see the future, the interface may prove foggier for consumers. The implementation turns the current smartphone operating system paradigm on its head, requiring a demo to fully comprehend how it’s different and why it’s useful. While I admit I wasn’t completely sold by the pitch, watching it in action brings its efficacy into sharp focus.\n",
      "The OS isn’t wholly disconnected from Google’s open operating system, but only in the sense that it’s built atop the Android kernel. As we’ve seen from the Trump-era development of Huawei’s HarmonyOS, it’s entirely possible to create something distinct from Android using that as a base. Here, generative AI is more than just integrated into the system, it’s the foundation to the way you interact with the device, how it responds and the interface it constructs.\n",
      "The notion of an “AI phone” isn’t an altogether new one. In fact, it’s a phrase you’re going to hear a lot in the coming years. I guarantee you’ll be sick of it by December. Elements of AI/ML have been integrated into devices in some form for several years now. Among other things, the technology is foundational to computational photography — that is the processing of the data collected by the camera sensor that occurs on the chip.\n",
      "Earlier this month, however, Samsung became one of the first large companies to really lean into the notion of an “AI phone.” The distinction here is the arrival of generative AI — the technology behind programs like Google Gemini and ChatGPT. Once again, much of the integration happens on the imaging side, but it’s beginning to filter into other aspects, as well.\n",
      "Image Credits: Brian Heater\n",
      "Given how big an investment Google has made in Gemini, it stands to reason that this trend will only ramp up in the coming years. Apple, too, will be entering the category at some time later this year. I wouldn’t classify generative AI as a complete gamechanger on these devices just yet, but it’s clear that those companies that don’t embrace it now are going to get left behind in the coming years.\n",
      "Brain.ai’s use of the technology goes much deeper than other current implementations. From a hardware perspective, however, it’s a standard smartphone. In fact, the Deutsch Telecom deal that found Yue exhibiting in the magenta-laden booth means the operating system will initially see the light of day via the device known as the T-Mobile REVVL here in the States (known as the “T Phone” in international markets like the EU). The precise model, release date and nature of the deal will be revealed “soon,” according to Yue.\n",
      "The truth, however, is that the Brain interface is designed to be hardware-agnostic, adapting to the form factor it’s been run on. That’s not to say that hardware isn’t important, of course. At its heart, the T-Mobile REVVL Plus, for example, is a budget phone, priced at around $200. It’s not a flagship by any stretch, but it gives you decent bang for your buck, including a Snapdragon 625 processor and dual rear camera at 13- and 15-megapixels, respectively. Although 2GB isn’t much RAM, Yue insists that the Brain.ai’s operating system can do more with less. Also, again, we don’t know what specific specs the device will have at launch.\n",
      "The interface begins with a static screen. From there, you query things off with either a voice or text prompt. In one example, Yue asks the system to “recommend a gift for my grandma, who can’t get out of bed.” From there, Brain goes to work pulling up not the response to the query, but an interface specific to it — in this case, it’s aggregated e-commerce results. The resulting page is barebones from a design perspective — black text on a white background. Sentences alternate with boxes showcasing results (in this case, blankets and Kindles).\n",
      "The query sits at the top. This, like much of the interface, is interactive. In this case, you can tap in to modify the search. Tapping on an image, meanwhile, will add it to a shopping cart for the third-party e-commerce site, and you can check out from there. I should note that all of the results in the demo were pulled directly from Amazon. Yue says the system will pull in some 7,000 retail sites at launch, and you can prioritize results by things like retailers and business size (if you’d prefer to support smaller businesses).\n",
      "Image Credits: Brian Heater\n",
      "Shopping is the first example Yue shows me, and many of the fundamentals apply across the board. Certainly there’s consistency in design across features. That’s due in large part to the fact that the device is actually devoid of third-party apps. This represents a massive shift from the current smartphone landscape for the past 15-plus years.\n",
      "“From a privacy and security perspective, we want to give a new level of control that people don’t have right now,” Yue. “The computer’s understanding of you, now it’s aggregated into different apps. These AI models are black boxes — recommendation machines that exploit our attention. We believe in explainable AI. We will be explaining to you, each step of the way, why we are making a recommendation. You have more people owning the AI and not big tech black boxes.”\n",
      "Adaptability is another big selling point. The model improves recommendations and gets more customized for the user the more queries are run and tweaked. Of course, third parties were the primary reason app stores revolutionized the industry. Suddenly you’ve gone from a single company creating all of your phone’s experiences to a system that harnesses the smarts and creativity of countless developers. Brain’s experience will be a combination of what its 100-person team can produce and what the AI model can dream up. As the model improves, so, too, will its functionality. Brain.ai is relying on its own model for the primary interface, but will pull from third parties like OpenAI and Google when it determines they’re better equipped to answer a specific query.\n",
      "Image Credits: Brian Heater\n",
      "There are limitations to what one can discover in a demo like this, so, as with many other elements, I’m going to have to wait until I have a shipping product in my hand to really evaluate the experience. I’m especially interested in how it handles certain applications, like imaging. It’s worth noting that the REVVL line doesn’t sport great cameras, so unless there’s a big upgrade, this won’t be the device for those who prioritize photos/videos.\n",
      "The camera will also play an important role in search. One example we discussed is taking a photo of a menu in a foreign country. Not only will it translate (à la Google Lens), it will also offer food recommendations based on your tastes. Yue also briefly demonstrated the system’s image generation with a simple request befitting our surroundings: make magenta sneakers. It did so quickly, with the only real bottleneck being convention center connection speeds (ironic, given the settings).\n",
      "Connectivity is vitally import here. The AI processing is being done off-device. I discussed the potential for adding some on-device processing, but Yue couldn’t confirm what it might look like at launch. Nor did I get an entirely clear answer for the offline experience. I suspect a big part of the reason Deutsch Telekom is so interested in the product is that it’s one that couldn’t exist in the same way without 5G. It recalls Mozilla’s ill-fated Firefox OS and the earliest days of Chrome OS, or any other number of examples of a product that loses significant functionality when offline.\n",
      "Image Credits: Brian Heater\n",
      "Yue founded Brain in 2015, and remained its sole employee until hiring a CTO the following year (Yue remains the sole founder). Born in China, he first connected to technology through a love of robotics and participation in the RoboCup robotic soccer tournament. At 18, he founded the Chinese social app, Friendoc. Two years later, he co-founded Benlai.com, which is now one of the country’s largest food delivery apps. Yue has since returned to the Bay Area to run Brain.ai full time. To date, the company has raised $80 million.\n",
      "After nearly a decade, the Brain interface is almost ready to launch — and it arrives at the perfect moment. The zeitgeist is very much focused on the manner of generative AI that powers the experience, from standalone devices like Rabbit and the Humane Ai Pin to tech giants like Samsung pitching their own “AI phones.”...\n"
     ]
    }
   ],
   "source": [
    "#Original Content for comparision\n",
    "index = 1\n",
    "if index >= 0 and index < len(headlines):\n",
    "    _, article_url = headlines[index]\n",
    "    content = get_article_content(article_url)\n",
    "    if content:\n",
    "        print(f\"\\n Title: {headlines[index][0]}\")\n",
    "        print(f\"\\n Content: {content[:]}...\")\n",
    "    else:\n",
    "        print(\"Content not available\")\n",
    "else:\n",
    "    print(\"Invalid index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47223a7c-b2df-4adf-9241-4be9f4147158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Content Length: 9622\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Paraphrased content block\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content:\n\u001b[1;32m----> 3\u001b[0m     paraphrased_content \u001b[38;5;241m=\u001b[39m \u001b[43mget_paraphrased_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m paraphrased_content:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mParaphrased Content:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 48\u001b[0m, in \u001b[0;36mget_paraphrased_content\u001b[1;34m(original_content)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[0;32m     47\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer([chunk], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 48\u001b[0m     summary_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     paraphrased_chunk \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(summary_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     50\u001b[0m     paraphrased_chunks\u001b[38;5;241m.\u001b[39mappend(paraphrased_chunk)\n",
      "File \u001b[1;32mD:\\ML Projects\\Blog-Bot\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ML Projects\\Blog-Bot\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1595\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1587\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1588\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1589\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   1590\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1591\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1592\u001b[0m     )\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[1;32m-> 1595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   1610\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1611\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1612\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1613\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1619\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1620\u001b[0m     )\n",
      "File \u001b[1;32mD:\\ML Projects\\Blog-Bot\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3294\u001b[0m, in \u001b[0;36mGenerationMixin.beam_sample\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3289\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[0;32m   3290\u001b[0m     next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3291\u001b[0m )  \u001b[38;5;66;03m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[0;32m   3293\u001b[0m next_token_scores_processed \u001b[38;5;241m=\u001b[39m logits_processor(input_ids, next_token_scores)\n\u001b[1;32m-> 3294\u001b[0m next_token_scores_processed \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_warper\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_scores_processed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3295\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m next_token_scores_processed \u001b[38;5;241m+\u001b[39m beam_scores[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mexpand_as(\n\u001b[0;32m   3296\u001b[0m     next_token_scores_processed\n\u001b[0;32m   3297\u001b[0m )\n\u001b[0;32m   3299\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ML Projects\\Blog-Bot\\venv\\Lib\\site-packages\\transformers\\generation\\logits_process.py:97\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mD:\\ML Projects\\Blog-Bot\\venv\\Lib\\site-packages\\transformers\\generation\\logits_process.py:510\u001b[0m, in \u001b[0;36mTopKLogitsWarper.__call__\u001b[1;34m(self, input_ids, scores)\u001b[0m\n\u001b[0;32m    508\u001b[0m top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Safety check\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# Remove all tokens with a probability less than the last token of the top-k\u001b[39;00m\n\u001b[1;32m--> 510\u001b[0m indices_to_remove \u001b[38;5;241m=\u001b[39m scores \u001b[38;5;241m<\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    511\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(indices_to_remove, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_value)\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Paraphrased content block\n",
    "if content:\n",
    "    paraphrased_content = get_paraphrased_content(content)\n",
    "    if paraphrased_content:\n",
    "        print(\"\\nParaphrased Content:\")\n",
    "        print(paraphrased_content)\n",
    "    else:\n",
    "        print(\"Paraphrased content not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c9cd58-26a3-4e74-9f23-e32dccb390d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
